{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import string\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from collections import Counter\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\automacao\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\automacao\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use some approaches for sentiment analysis in order to study their performances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "### Large Movie Review Dataset\n",
    "Downloaded from http://ai.stanford.edu/~amaas/data/sentiment/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "- [Approach 1](#approach1): Bag of Words (word **occurrences**)\n",
    "    - Sklearn CountVectorizer (NLTK stopwords) -> Multinomial Naive Bayes\n",
    "    - [Result](#approach1_result)\n",
    "    \n",
    "    \n",
    "- [Approach 2](#approach2): Bag of Words (word **frequencies**)\n",
    "    - Sklearn CountVectorizer (NLTK stopwords) -> TfidfTransformer (**without IDF**) -> Multinomial Naive Bayes\n",
    "    - [Result](#approach2_result)\n",
    "    \n",
    "    \n",
    "- [Approach 3](#approach3): Bag of Words (word **frequencies** + inverse document frequencies - TF-IDF)\n",
    "    - Sklearn CountVectorizer (NLTK stopwords) -> TfidfTransformer (**with IDF**) -> Multinomial Naive Bayes\n",
    "    - [Result](#approach3_result)\n",
    "    \n",
    "\n",
    "- [Approach 4](#approach4): Bag of Words (word **occurrences**)\n",
    "    - Sklearn CountVectorizer (NLTK stopwords) -> Bernoulli Naive Bayes\n",
    "    - [Result](#approach4_result)\n",
    "\n",
    "\n",
    "- [Approach 5](#approach5): Bag of Words (word **frequencies**)\n",
    "    - Sklearn CountVectorizer (NLTK stopwords + min/maxdf) -> TfidfTransformer (**without IDF**) -> Multinomial Naive Bayes\n",
    "    - [Result](#approach5_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO - approaches:\n",
    "- Use CountVectorizer with custom NLTK processing.\n",
    "- NLTK preprocessing and numpy arrays. Problem: data does not fit in memory.\n",
    "- NLTK preprocessing and incremental learning with naive bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='approach1'></a>\n",
    "# Approach 1\n",
    "\n",
    "#### Bag Of Words (word occurrences)\n",
    "Sklearn CountVectorizer (NLTK stopwords) -> Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_train_raw_folder = (Path('..') / 'data' / 'aclImdb_v1/aclImdb/train/pos').resolve()\n",
    "neg_train_raw_folder = (Path('..') / 'data' / 'aclImdb_v1/aclImdb/train/neg').resolve()\n",
    "pos_test_raw_folder = (Path('..') / 'data' / 'aclImdb_v1/aclImdb/test/pos').resolve()\n",
    "neg_test_raw_folder = (Path('..') / 'data' / 'aclImdb_v1/aclImdb/test/neg').resolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read train data:\n",
    "pos_train_raw = list()\n",
    "for file in pos_train_raw_folder.iterdir():\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        pos_train_raw.append(f.read())\n",
    "neg_train_raw = list()\n",
    "for file in neg_train_raw_folder.iterdir():\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        neg_train_raw.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read test data:\n",
    "pos_test_raw = list()\n",
    "for file in pos_test_raw_folder.iterdir():\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        pos_test_raw.append(f.read())\n",
    "\n",
    "neg_test_raw = list()\n",
    "for file in neg_test_raw_folder.iterdir():\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        neg_test_raw.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countvec = CountVectorizer(stop_words=nltk.corpus.stopwords.words('english'), binary=True).fit(pos_train_raw + neg_train_raw)\n",
    "\n",
    "#Prepare train data:\n",
    "X_train = countvec.transform(pos_train_raw + neg_train_raw)\n",
    "\n",
    "n_pos_samples_train = len(pos_train_raw)\n",
    "n_neg_samples_train = len(neg_train_raw)\n",
    "y_train = np.concatenate((np.ones((n_pos_samples_train)), np.zeros((n_neg_samples_train))))\n",
    "\n",
    "#Fit model:\n",
    "naivebayes = MultinomialNB()\n",
    "naivebayes.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='approach1_result'></a>\n",
    "#### Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.872\n",
      "Recall: 0.7862\n",
      "Accuracy: 0.8354\n"
     ]
    }
   ],
   "source": [
    "#Prepare test data:\n",
    "n_pos_samples_test = len(pos_test_raw)\n",
    "n_neg_samples_test = len(neg_test_raw)\n",
    "\n",
    "X_test = countvec.transform(pos_test_raw + neg_test_raw)\n",
    "y_test = np.concatenate((np.ones((n_pos_samples_test)), np.zeros((n_neg_samples_test))))\n",
    "y_pred = naivebayes.predict(X_test)\n",
    "\n",
    "print('Precision: {}'.format(np.round(precision_score(y_test, y_pred), decimals=4)))\n",
    "print('Recall: {}'.format(np.round(recall_score(y_test, y_pred), decimals=4)))\n",
    "print('Accuracy: {}'.format(np.round(accuracy_score(y_test, y_pred), decimals=4)))\n",
    "\n",
    "#Generate list of results for visual comparison:\n",
    "results = list()\n",
    "results.append({\n",
    "    'precision': precision_score(y_test, y_pred),\n",
    "    'recall': recall_score(y_test, y_pred),\n",
    "    'accuracy': accuracy_score(y_test, y_pred)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='approach2'></a>\n",
    "# Approach 2\n",
    "\n",
    "#### Bag Of Words (word frequencies)\n",
    "Sklearn CountVectorizer (NLTK stopwords) -> TfidfTransformer (without IDF) -> Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_train_raw_folder = (Path('..') / 'data' / 'aclImdb_v1/aclImdb/train/pos').resolve()\n",
    "neg_train_raw_folder = (Path('..') / 'data' / 'aclImdb_v1/aclImdb/train/neg').resolve()\n",
    "pos_test_raw_folder = (Path('..') / 'data' / 'aclImdb_v1/aclImdb/test/pos').resolve()\n",
    "neg_test_raw_folder = (Path('..') / 'data' / 'aclImdb_v1/aclImdb/test/neg').resolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read train data:\n",
    "pos_train_raw = list()\n",
    "for file in pos_train_raw_folder.iterdir():\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        pos_train_raw.append(f.read())\n",
    "neg_train_raw = list()\n",
    "for file in neg_train_raw_folder.iterdir():\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        neg_train_raw.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read test data:\n",
    "pos_test_raw = list()\n",
    "for file in pos_test_raw_folder.iterdir():\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        pos_test_raw.append(f.read())\n",
    "\n",
    "neg_test_raw = list()\n",
    "for file in neg_test_raw_folder.iterdir():\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        neg_test_raw.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countvec = CountVectorizer(stop_words=nltk.corpus.stopwords.words('english')).fit(pos_train_raw + neg_train_raw)\n",
    "\n",
    "#Prepare train data:\n",
    "X_train = countvec.transform(pos_train_raw + neg_train_raw)\n",
    "#Using term frequency tranformer:\n",
    "tfidf = TfidfTransformer(use_idf=False).fit(X_train)\n",
    "X_train = tfidf.transform(X_train)\n",
    "\n",
    "n_pos_samples_train = len(pos_train_raw)\n",
    "n_neg_samples_train = len(neg_train_raw)\n",
    "y_train = np.concatenate((np.ones((n_pos_samples_train)), np.zeros((n_neg_samples_train))))\n",
    "\n",
    "#Fit model:\n",
    "naivebayes = MultinomialNB()\n",
    "naivebayes.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='approach2_result'></a>\n",
    "#### Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.876\n",
      "Recall: 0.8134\n",
      "Accuracy: 0.8492\n"
     ]
    }
   ],
   "source": [
    "#Prepare test data:\n",
    "n_pos_samples_test = len(pos_test_raw)\n",
    "n_neg_samples_test = len(neg_test_raw)\n",
    "\n",
    "X_test = countvec.transform(pos_test_raw + neg_test_raw)\n",
    "X_test = tfidf.transform(X_test)\n",
    "y_test = np.concatenate((np.ones((n_pos_samples_test)), np.zeros((n_neg_samples_test))))\n",
    "y_pred = naivebayes.predict(X_test)\n",
    "\n",
    "print('Precision: {}'.format(np.round(precision_score(y_test, y_pred), decimals=4)))\n",
    "print('Recall: {}'.format(np.round(recall_score(y_test, y_pred), decimals=4)))\n",
    "print('Accuracy: {}'.format(np.round(accuracy_score(y_test, y_pred), decimals=4)))\n",
    "\n",
    "results.append({\n",
    "    'precision': precision_score(y_test, y_pred),\n",
    "    'recall': recall_score(y_test, y_pred),\n",
    "    'accuracy': accuracy_score(y_test, y_pred)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='approach3'></a>\n",
    "# Approach 3\n",
    "\n",
    "#### Bag Of Words (word frequencies + inverse document frequency - IDF)\n",
    "Sklearn CountVectorizer (NLTK stopwords) -> TfidfTransformer (with IDF) -> Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_train_raw_folder = (Path('..') / 'data' / 'aclImdb_v1/aclImdb/train/pos').resolve()\n",
    "neg_train_raw_folder = (Path('..') / 'data' / 'aclImdb_v1/aclImdb/train/neg').resolve()\n",
    "pos_test_raw_folder = (Path('..') / 'data' / 'aclImdb_v1/aclImdb/test/pos').resolve()\n",
    "neg_test_raw_folder = (Path('..') / 'data' / 'aclImdb_v1/aclImdb/test/neg').resolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read train data:\n",
    "pos_train_raw = list()\n",
    "for file in pos_train_raw_folder.iterdir():\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        pos_train_raw.append(f.read())\n",
    "neg_train_raw = list()\n",
    "for file in neg_train_raw_folder.iterdir():\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        neg_train_raw.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read test data:\n",
    "pos_test_raw = list()\n",
    "for file in pos_test_raw_folder.iterdir():\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        pos_test_raw.append(f.read())\n",
    "\n",
    "neg_test_raw = list()\n",
    "for file in neg_test_raw_folder.iterdir():\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        neg_test_raw.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countvec = CountVectorizer(stop_words=nltk.corpus.stopwords.words('english')).fit(pos_train_raw + neg_train_raw)\n",
    "\n",
    "#Prepare train data:\n",
    "X_train = countvec.transform(pos_train_raw + neg_train_raw)\n",
    "#Using term frequency tranformer:\n",
    "tfidf = TfidfTransformer(use_idf=True).fit(X_train)\n",
    "X_train = tfidf.transform(X_train)\n",
    "\n",
    "n_pos_samples_train = len(pos_train_raw)\n",
    "n_neg_samples_train = len(neg_train_raw)\n",
    "y_train = np.concatenate((np.ones((n_pos_samples_train)), np.zeros((n_neg_samples_train))))\n",
    "\n",
    "#Fit model:\n",
    "naivebayes = MultinomialNB()\n",
    "naivebayes.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='approach3_result'></a>\n",
    "#### Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8687\n",
      "Recall: 0.7875\n",
      "Accuracy: 0.8342\n"
     ]
    }
   ],
   "source": [
    "#Prepare test data:\n",
    "n_pos_samples_test = len(pos_test_raw)\n",
    "n_neg_samples_test = len(neg_test_raw)\n",
    "\n",
    "X_test = countvec.transform(pos_test_raw + neg_test_raw)\n",
    "X_test = tfidf.transform(X_test)\n",
    "y_test = np.concatenate((np.ones((n_pos_samples_test)), np.zeros((n_neg_samples_test))))\n",
    "y_pred = naivebayes.predict(X_test)\n",
    "\n",
    "print('Precision: {}'.format(np.round(precision_score(y_test, y_pred), decimals=4)))\n",
    "print('Recall: {}'.format(np.round(recall_score(y_test, y_pred), decimals=4)))\n",
    "print('Accuracy: {}'.format(np.round(accuracy_score(y_test, y_pred), decimals=4)))\n",
    "\n",
    "results.append({\n",
    "    'precision': precision_score(y_test, y_pred),\n",
    "    'recall': recall_score(y_test, y_pred),\n",
    "    'accuracy': accuracy_score(y_test, y_pred)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='approach4'></a>\n",
    "# Approach 4\n",
    "\n",
    "#### Bag Of Words (word occurrences)\n",
    "Sklearn CountVectorizer (NLTK stopwords) -> Bernoulli Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_train_raw_folder = (Path('..') / 'data' / 'aclImdb_v1/aclImdb/train/pos').resolve()\n",
    "neg_train_raw_folder = (Path('..') / 'data' / 'aclImdb_v1/aclImdb/train/neg').resolve()\n",
    "pos_test_raw_folder = (Path('..') / 'data' / 'aclImdb_v1/aclImdb/test/pos').resolve()\n",
    "neg_test_raw_folder = (Path('..') / 'data' / 'aclImdb_v1/aclImdb/test/neg').resolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read train data:\n",
    "pos_train_raw = list()\n",
    "for file in pos_train_raw_folder.iterdir():\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        pos_train_raw.append(f.read())\n",
    "neg_train_raw = list()\n",
    "for file in neg_train_raw_folder.iterdir():\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        neg_train_raw.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read test data:\n",
    "pos_test_raw = list()\n",
    "for file in pos_test_raw_folder.iterdir():\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        pos_test_raw.append(f.read())\n",
    "\n",
    "neg_test_raw = list()\n",
    "for file in neg_test_raw_folder.iterdir():\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        neg_test_raw.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB()"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countvec = CountVectorizer(stop_words=nltk.corpus.stopwords.words('english'), binary=True).fit(pos_train_raw + neg_train_raw)\n",
    "\n",
    "#Prepare train data:\n",
    "X_train = countvec.transform(pos_train_raw + neg_train_raw)\n",
    "\n",
    "n_pos_samples_train = len(pos_train_raw)\n",
    "n_neg_samples_train = len(neg_train_raw)\n",
    "y_train = np.concatenate((np.ones((n_pos_samples_train)), np.zeros((n_neg_samples_train))))\n",
    "\n",
    "#Fit model:\n",
    "naivebayes = BernoulliNB()\n",
    "naivebayes.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='approach4_result'></a>\n",
    "#### Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8734\n",
      "Recall: 0.7442\n",
      "Accuracy: 0.8182\n"
     ]
    }
   ],
   "source": [
    "#Prepare test data:\n",
    "n_pos_samples_test = len(pos_test_raw)\n",
    "n_neg_samples_test = len(neg_test_raw)\n",
    "\n",
    "X_test = countvec.transform(pos_test_raw + neg_test_raw)\n",
    "y_test = np.concatenate((np.ones((n_pos_samples_test)), np.zeros((n_neg_samples_test))))\n",
    "y_pred = naivebayes.predict(X_test)\n",
    "\n",
    "print('Precision: {}'.format(np.round(precision_score(y_test, y_pred), decimals=4)))\n",
    "print('Recall: {}'.format(np.round(recall_score(y_test, y_pred), decimals=4)))\n",
    "print('Accuracy: {}'.format(np.round(accuracy_score(y_test, y_pred), decimals=4)))\n",
    "\n",
    "results.append({\n",
    "    'precision': precision_score(y_test, y_pred),\n",
    "    'recall': recall_score(y_test, y_pred),\n",
    "    'accuracy': accuracy_score(y_test, y_pred)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='approach5'></a>\n",
    "# Approach 5\n",
    "\n",
    "#### Bag Of Words (word frequencies)\n",
    "Sklearn CountVectorizer (NLTK stopwords + min/maxdf) -> TfidfTransformer (**without IDF**) -> Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_train_raw_folder = (Path('..') / 'data' / 'aclImdb_v1/aclImdb/train/pos').resolve()\n",
    "neg_train_raw_folder = (Path('..') / 'data' / 'aclImdb_v1/aclImdb/train/neg').resolve()\n",
    "pos_test_raw_folder = (Path('..') / 'data' / 'aclImdb_v1/aclImdb/test/pos').resolve()\n",
    "neg_test_raw_folder = (Path('..') / 'data' / 'aclImdb_v1/aclImdb/test/neg').resolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read train data:\n",
    "pos_train_raw = list()\n",
    "for file in pos_train_raw_folder.iterdir():\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        pos_train_raw.append(f.read())\n",
    "neg_train_raw = list()\n",
    "for file in neg_train_raw_folder.iterdir():\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        neg_train_raw.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read test data:\n",
    "pos_test_raw = list()\n",
    "for file in pos_test_raw_folder.iterdir():\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        pos_test_raw.append(f.read())\n",
    "\n",
    "neg_test_raw = list()\n",
    "for file in neg_test_raw_folder.iterdir():\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        neg_test_raw.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB()"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countvec = CountVectorizer(stop_words=nltk.corpus.stopwords.words('english'), binary=False, \n",
    "                          min_df=0.005, max_df=0.98).fit(pos_train_raw + neg_train_raw)\n",
    "\n",
    "#Prepare train data:\n",
    "X_train = countvec.transform(pos_train_raw + neg_train_raw)\n",
    "tfidf = TfidfTransformer(use_idf=False).fit(X_train)\n",
    "X_train = tfidf.transform(X_train)\n",
    "\n",
    "n_pos_samples_train = len(pos_train_raw)\n",
    "n_neg_samples_train = len(neg_train_raw)\n",
    "y_train = np.concatenate((np.ones((n_pos_samples_train)), np.zeros((n_neg_samples_train))))\n",
    "\n",
    "#Fit model:\n",
    "naivebayes = BernoulliNB()\n",
    "naivebayes.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='approach5_result'></a>\n",
    "#### Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8544\n",
      "Recall: 0.8395\n",
      "Accuracy: 0.8482\n"
     ]
    }
   ],
   "source": [
    "#Prepare test data:\n",
    "n_pos_samples_test = len(pos_test_raw)\n",
    "n_neg_samples_test = len(neg_test_raw)\n",
    "\n",
    "X_test = countvec.transform(pos_test_raw + neg_test_raw)\n",
    "X_test = tfidf.transform(X_test)\n",
    "y_test = np.concatenate((np.ones((n_pos_samples_test)), np.zeros((n_neg_samples_test))))\n",
    "y_pred = naivebayes.predict(X_test)\n",
    "\n",
    "print('Precision: {}'.format(np.round(precision_score(y_test, y_pred), decimals=4)))\n",
    "print('Recall: {}'.format(np.round(recall_score(y_test, y_pred), decimals=4)))\n",
    "print('Accuracy: {}'.format(np.round(accuracy_score(y_test, y_pred), decimals=4)))\n",
    "\n",
    "results.append({\n",
    "    'precision': precision_score(y_test, y_pred),\n",
    "    'recall': recall_score(y_test, y_pred),\n",
    "    'accuracy': accuracy_score(y_test, y_pred)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom preprocessing with NLTK approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text, language='english'):\n",
    "    #Lower case -> remove punctuation -> tokenization -> remove stopwords\n",
    "    out = text.lower()\n",
    "    out = \"\".join([char for char in out if char not in string.punctuation])\n",
    "    out = nltk.word_tokenize(out)\n",
    "    out = [word for word in out if word not in nltk.corpus.stopwords.words(language)]\n",
    "    stemer = PorterStemmer()\n",
    "    out = [stemer.stem(word) for word in out]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing to generate tokens:\n",
    "pos_train = list()\n",
    "for text in pos_train_raw:\n",
    "    pos_train.append(preprocess(text))\n",
    "neg_train = list()\n",
    "for text in neg_train_raw:\n",
    "    neg_train.append(preprocess(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the vocab set:\n",
    "vocab = set()\n",
    "for tokens in pos_train:\n",
    "    for token in tokens:\n",
    "        vocab.add(token)\n",
    "for tokens in neg_train:\n",
    "    for token in tokens:\n",
    "        vocab.add(token)\n",
    "\n",
    "#Generate the mapping (word -> position in X), (position in X -> word):\n",
    "position_word = dict()\n",
    "word_position = dict()\n",
    "count = 0\n",
    "for word in vocab:\n",
    "    position_word[count] = word\n",
    "    word_position[word] = count\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize X and y:\n",
    "n_pos_samples = len(pos_train)\n",
    "n_neg_samples = len(neg_train)\n",
    "n_vocab = len(vocab)\n",
    "X = np.zeros((n_pos_samples + n_neg_samples, n_vocab), dtype=np.int8)\n",
    "y = np.concatenate((np.ones((n_pos_samples, 1)), np.zeros((n_neg_samples, 1))))\n",
    "\n",
    "#Generate the X matrix with word counts:\n",
    "for idx, tokens in enumerate(pos_train + neg_train):\n",
    "    word_counts = Counter(tokens)\n",
    "    for word in word_counts.keys():\n",
    "        X[idx, word_position[word]] = word_counts[word]\n",
    "\n",
    "#TODO - option for word occurrences\n",
    "#TODO - option for TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 2.16 GiB for an array with shape (25000, 92756) and data type int8",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-258-83cc38ca3965>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcsr_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_pos_samples\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mn_neg_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_vocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 2.16 GiB for an array with shape (25000, 92756) and data type int8"
     ]
    }
   ],
   "source": [
    "csr_matrix(np.zeros((n_pos_samples + n_neg_samples, n_vocab), dtype=np.int8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 238. MiB for an array with shape (25000, 10000) and data type int8",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-267-a6f9e57e0b7f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_pos_samples\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mn_neg_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 238. MiB for an array with shape (25000, 10000) and data type int8"
     ]
    }
   ],
   "source": [
    "np.zeros((n_pos_samples + n_neg_samples, 10000), dtype=np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "naivebayes = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-237-e41d510a2820>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnaivebayes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "naivebayes.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_train_tokens = list()\n",
    "for tokens in pos_train:\n",
    "    string = ''\n",
    "    for token in tokens:\n",
    "        string = string + token + ' '\n",
    "    string = string.strip()\n",
    "    pos_train_tokens.append(string)\n",
    "\n",
    "neg_train_tokens = list()\n",
    "for tokens in neg_train:\n",
    "    string = ''\n",
    "    for token in tokens:\n",
    "        string = string + token + ' '\n",
    "    string = string.strip()\n",
    "    neg_train_tokens.append(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countvec = CountVectorizer()\n",
    "countvec.fit(pos_train_tokens + neg_train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\movie-sentiment-analysis\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "C:\\ProgramData\\Anaconda3\\envs\\movie-sentiment-analysis\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['arent', 'couldnt', 'didnt', 'doesnt', 'dont', 'hadnt', 'hasnt', 'havent', 'isnt', 'mightnt', 'mustnt', 'neednt', 'shant', 'shes', 'shouldnt', 'shouldve', 'thatll', 'wasnt', 'werent', 'wont', 'wouldnt', 'youd', 'youll', 'youre', 'youve'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "def tokenize(text):\n",
    "    text = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "countvec = CountVectorizer(tokenizer=tokenize, stop_words=nltk.corpus.stopwords.words('english'))\n",
    "countvec.fit(pos_train_raw + neg_train_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 74704)"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countvec.transform(pos_train_raw + neg_train_raw).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 92345)"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countvec.transform(pos_train_tokens + neg_train_tokens).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it\n",
      "enemies\n",
      "homosexuality\n",
      "nights\n",
      "ll\n",
      "credits\n",
      "standards\n",
      "fantastic\n",
      "wouldn\n",
      "grotesques\n",
      "at\n",
      "cutting\n",
      "onli\n",
      "props\n",
      "swings\n",
      "crocodiles\n",
      "ones\n",
      "conventions\n",
      "nature\n",
      "films\n",
      "leads\n",
      "herself\n",
      "tari\n",
      "society\n",
      "filmed\n",
      "violence\n",
      "picture\n",
      "russwil\n",
      "beckinsale\n",
      "suitors\n",
      "the\n",
      "didn\n",
      "kidnapped\n",
      "imagery\n",
      "lik\n",
      "coincidentally\n",
      "since\n",
      "ambiguity\n",
      "knowing\n",
      "ambiguous\n",
      "peaceful\n",
      "vance\n",
      "1940th\n",
      "lives\n",
      "for\n",
      "than\n",
      "adequate\n",
      "ordered\n",
      "oherlihy\n",
      "progressive\n",
      "filipinos\n",
      "reactions\n",
      "again\n",
      "springs\n",
      "greatness\n",
      "biographies\n",
      "surprise\n",
      "direction\n",
      "family\n",
      "1987al\n",
      "particulare\n",
      "isn\n",
      "apache\n",
      "illinois\n",
      "1940bi\n",
      "peli\n",
      "upside\n",
      "or\n",
      "epochs\n",
      "times\n",
      "virtuously\n",
      "pac\n",
      "popeye\n",
      "rivals\n",
      "won\n",
      "orna\n",
      "bubblebr\n",
      "simple\n",
      "obfuscated\n",
      "television\n",
      "doesn\n",
      "dramas\n",
      "characters\n",
      "assembled\n",
      "officials\n",
      "sso\n",
      "briainbr\n",
      "himself\n",
      "such\n",
      "tdefinit\n",
      "wasn\n",
      "49484\n",
      "13516\n",
      "1697\n",
      "oppression\n",
      "looks\n",
      "079\n",
      "images\n",
      "ain\n",
      "history\n",
      "boheme\n",
      "medical\n",
      "episode\n",
      "future\n",
      "lookalike\n",
      "kids\n",
      "wel\n",
      "parenting\n",
      "requires\n",
      "travesty\n",
      "brant\n",
      "polarbear\n",
      "unflinchingly\n",
      "mentions\n",
      "hasn\n",
      "possessing\n",
      "veri\n",
      "impossible\n",
      "galaxy\n",
      "tried\n",
      "crazy\n",
      "people\n",
      "brilli\n",
      "melville\n",
      "anthology\n",
      "nudity\n",
      "cyborgs\n",
      "features\n",
      "ending\n",
      "actingthor\n",
      "zombies\n",
      "nobody\n",
      "stallone\n",
      "480\n",
      "are\n",
      "titles\n",
      "is\n",
      "same\n",
      "decisions\n",
      "costumes\n",
      "sheets\n",
      "confusing\n",
      "college\n",
      "series\n",
      "funny\n",
      "minutes\n",
      "brut\n",
      "1775\n",
      "toys\n",
      "corpse\n",
      "18000000\n",
      "tampax\n",
      "sabretooth2002\n",
      "bc2006\n",
      "atrocious\n",
      "shouldn\n",
      "secrets\n",
      "ashamed\n",
      "then\n",
      "leave\n",
      "warned\n"
     ]
    }
   ],
   "source": [
    "teste = list()\n",
    "for word in countvec.vocabulary_.keys():\n",
    "    if word not in vocab:\n",
    "        print(word)\n",
    "        teste.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "bubblebr\n",
      "forest¨1936\n",
      "conventionsa\n",
      "wellpac\n",
      "lot´\n",
      "bg´\n",
      "you´v\n",
      "a\n",
      "character´\n",
      "parentingwher\n",
      "endingin\n",
      "loserto\n",
      "maybe´\n",
      "even\n",
      "§1000\n",
      "shorti\n",
      "andalthough\n",
      "ladybug´\n",
      "stefan´\n",
      "moviejust\n",
      "moviebecaus\n",
      "£399\n",
      "¡§rocket\n",
      "£200\n",
      "alonzo\n",
      "£250\n",
      "lugosiyet\n",
      "¨thousand\n",
      "i´d\n",
      "thirdrat\n",
      "don´t\n",
      "¨jurassik\n",
      "»\n",
      "we´ll\n",
      "tribesmenthu\n",
      "it´d\n",
      "¨zane\n",
      "ambiguitythi\n",
      "scream\n",
      "k\n",
      "what´\n",
      "creditsand\n",
      "projectbut\n",
      "70£\n",
      "youthi\n",
      "pitt´\n",
      "polarbearand\n",
      "old\n",
      "tamer¨\n",
      "nobudget\n",
      "deadonli\n",
      "dalmatian\n",
      "beckinsale´\n",
      "¡§just\n",
      "4\n",
      "£9\n",
      "fulci´\n",
      "§12\n",
      "method\n",
      "you´ll\n",
      "¾\n",
      "them\n",
      "big\n",
      "2004¡¨\n",
      "wellmostli\n",
      "problembrilli\n",
      "guyhi\n",
      "futuremor\n",
      "nearand\n",
      "butmom\n",
      "movieand\n",
      "kidnappedin\n",
      "brant´\n",
      "£1775\n",
      "hima\n",
      "womenadela\n",
      "oherlihyto\n",
      "deathit\n",
      "insignific\n",
      "price…but\n",
      "wagoncomplet\n",
      "“\n",
      "attictherel\n",
      "riemann´\n",
      "director¡¦\n",
      "b\n",
      "w\n",
      "token\n",
      "when\n",
      "99¢\n",
      "doesn´t\n",
      "morganbut\n",
      "featuresthat\n",
      "zombiesnatch\n",
      "pointjust\n",
      "fear\n",
      "enemiesth\n",
      "upsidedownor\n",
      "60´\n",
      "orna\n",
      "rivalskeaton\n",
      "cgiwhich\n",
      "macarthurlik\n",
      "episodeher\n",
      "didn´t\n",
      "hammi\n",
      "¨petrifi\n",
      "¡§at\n",
      "caus\n",
      "3\n",
      "obfuscatedthread\n",
      "he´\n",
      "atrociousth\n",
      "charactersolivia\n",
      "binso\n",
      "hasn´tbr\n",
      "wholik\n",
      "targetenerv\n",
      "timeand\n",
      "movi\n",
      "manif\n",
      "assembled\n",
      "longlost\n",
      "®\n",
      "¿rememb\n",
      "¨gore\n",
      "psycho\n",
      "sideand\n",
      "nudityal\n",
      "same\n",
      "£20\n",
      "movieson\n",
      "attichey\n",
      "film´\n",
      "peak…\n",
      "collegego\n",
      "gu¨¦tari\n",
      "£079\n",
      "funnyso\n",
      "pasolini´\n",
      "o´briainbr\n",
      "£350\n",
      "kidswel\n",
      "boatthu\n",
      "tasuieva\n",
      "£500000\n",
      "year´\n",
      "rothth\n",
      "grotesquesat\n",
      "«\n",
      "swingsbut\n",
      "scientistilona\n",
      "president´\n",
      "can´t\n",
      "it¡¦\n",
      "timeso\n",
      "talenam\n",
      "melville´\n",
      "filmedan\n",
      "¨scandal\n",
      "violencememor\n",
      "ambiguousth\n",
      "chan´\n",
      "truth\n",
      "natureth\n",
      "highh\n",
      "deaf\n",
      "£1000\n",
      "berlin´\n",
      "humoran\n",
      "´till\n",
      "rave\n",
      "¡§galaxy¡¨\n",
      "unflinchinglywhat\n",
      "crocodilesth\n",
      "orderedbi\n",
      "filmwel\n",
      "couplea\n",
      "tampax®\n",
      "pilmark´\n",
      "v\n",
      "15yearold\n",
      "peliki\n",
      "round\n",
      "thurmansh\n",
      "bad\n",
      "approachkeaton\n",
      "castamong\n",
      "hybridnot\n",
      "i´v\n",
      "women´\n",
      "girl´\n",
      "chestruggl\n",
      "adequateand\n",
      "5\n",
      "movieeven\n",
      "epochsin\n",
      "curiou\n",
      "shouldn´t\n",
      "apache¨\n",
      "wasn´t\n",
      "leaveev\n",
      "trialat\n",
      "l\n",
      "she´\n",
      "day¨\n",
      "batman\n",
      "usi\n",
      "¨a\n",
      "charact\n",
      "cameron´\n",
      "g\n",
      "¨town\n",
      "£49484\n",
      "musicbut\n",
      "it´sso\n",
      "but\n",
      "jrare\n",
      "¨the\n",
      "palac\n",
      "badif\n",
      "austen´\n",
      "arabella\n",
      "aro´\n",
      "decisionsin\n",
      "desksymbol\n",
      "european´\n",
      "quit\n",
      "homer¡¦\n",
      "originali\n",
      "isnot\n",
      "£7br\n",
      "£230\n",
      "park¨\n",
      "¡viva\n",
      "boheme¨\n",
      "doña\n",
      "dramasar\n",
      "popeyeliketh\n",
      "£800\n",
      "mentionsray\n",
      "retir\n",
      "hereit\n",
      "dougand\n",
      "husbandgino\n",
      "surprisethrough\n",
      "lubitsch\n",
      "possessingand\n",
      "warnedit\n",
      "comparisonar\n",
      "¡the\n",
      "harlin´\n",
      "minutesbrut\n",
      "lemerci\n",
      "springsthi\n",
      "filipinos\n",
      "scenesth\n",
      "order\n",
      "televisionor\n",
      "¡§octob\n",
      "town´\n",
      "expertjewelri\n",
      "predat\n",
      "greatnessand\n",
      "let´\n",
      "waxwork\n",
      "sky¡¨\n",
      "grey¨\n",
      "seidl´\n",
      "goldi\n",
      "sweet\n",
      "à\n",
      "\b\b\b\ba\n",
      "·\n",
      "knowingi\n",
      "grotesqu\n",
      "¿actingthor\n",
      "¨\n",
      "sexa\n",
      "kobayashi¡¦\n",
      "e\n",
      "witherspoonsh\n",
      "imagesi\n",
      "cataluña´\n",
      "we´v\n",
      "”\n",
      "\u0010own\n",
      "oscar®\n",
      "titlesy\n",
      "¿special\n",
      "£499\n",
      "cyborgsrobot\n",
      "£500\n",
      "moment\n",
      "veri\n",
      "funit\n",
      "£10\n",
      "youngestand\n",
      "illinois¨1940bi\n",
      "hereand\n",
      "alongno\n",
      "wwiino\n",
      "spiritu\n",
      "uptod\n",
      "¡§astronaut\n",
      "medicalgenet\n",
      "nobody´\n",
      "that\n",
      "–\n",
      "filmswer\n",
      "officialsal\n",
      "sighta\n",
      "funyet\n",
      "whichlegend\n",
      "¨le\n",
      "onenot\n",
      "2007an\n",
      "it´\n",
      "lincoln¨\n",
      "z\n",
      "seriesal\n",
      "£2000\n",
      "requiresa\n",
      "i´m\n",
      "reactionsagain\n",
      "you´r\n",
      "£1697\n",
      "fifth\n",
      "pinkth\n",
      "cast´\n",
      "¨10000\n",
      "the\n",
      "womennon\n",
      "4¨una\n",
      "russwil\n",
      "£300\n",
      "people¡¦\n",
      "£5\n",
      "returnhi\n",
      "p\n",
      "thema\n",
      "1the\n",
      "¨abe\n",
      "discov\n",
      "manfr\n",
      "family¨1987al\n",
      "crap\n",
      "\n",
      "thenwel\n",
      "£4\n",
      "organ\n",
      "don¡¦t\n",
      "\n",
      "wellknown\n",
      "womanand\n",
      "subject\n",
      "\n",
      "fatefirst\n",
      "£299\n",
      "bubbl\n",
      "herselfthat\n",
      "conventexploit\n",
      "1982he\n",
      "itcan\n",
      "’\n",
      "illus\n",
      "there´\n",
      "himand\n",
      "8\n",
      "¨abraham\n",
      "friend\n",
      "5\n",
      "himbut\n",
      "1\n",
      "free\n",
      "progressivecommand\n",
      "2¨nuit\n",
      "£400\n",
      "thomerson´\n",
      "moviemi\n",
      "st\n",
      "j\n",
      "cartoonish\n",
      "¡§crazy¡¨\n",
      "pregnantwhat\n",
      "nights¨\n",
      "withbedlam\n",
      "´dollman\n",
      "r\n",
      "corpseth\n",
      "allth\n",
      "£3\n",
      "bantereven\n",
      "£13516\n",
      "¡gracia\n",
      "19881992\n",
      "children´\n",
      "6\n",
      "2000it\n",
      "like\n",
      "himselfsuch\n",
      "enoughand\n",
      "u\n",
      "£150\n",
      "reallyit´\n",
      "s\n",
      "lifeth\n",
      "¨big\n",
      "1214\n",
      "bestbut\n",
      "¡§but\n",
      "h\n",
      "¡§impossible¡¨\n",
      "byambit\n",
      "¡colombian\n",
      "historybut\n",
      "stallonethat\n",
      "½\n",
      "0\n",
      "autumn\n",
      "patric´\n",
      "ain´t\n",
      "giovannaar\n",
      "moviesth\n",
      "\n",
      "7\n",
      "brainless\n",
      "f\n",
      "boy¡¨\n",
      "o\n",
      "₤100\n",
      "haven´t\n",
      "le\n",
      "suitors…although\n",
      "aboutan\n",
      "week´\n",
      "seena\n",
      "¨grape\n",
      "virtuouslypac\n",
      "agekudo\n",
      "bmovi\n",
      "´co\n",
      "toys´\n",
      "£50\n",
      "bal¨1982\n",
      "hellbut\n",
      "since…\n",
      "philipa\n",
      "whocoincidentallyresembl\n",
      "dramafor\n",
      "knit\n",
      "fantastichi\n",
      "leadsgino\n",
      "they´r\n",
      "brilliantli\n",
      "£480\n",
      "andbest\n",
      "shoot\n",
      "backther\n",
      "usstil\n",
      "¨sabretooth2002¨bi\n",
      "andrea\n",
      "marchth\n",
      "thata\n",
      "tried¡¨\n",
      "wouldn´t\n",
      "whichev\n",
      "livesfor\n",
      "secretsdirector\n",
      "cupido\n",
      "wall\n",
      "on\n",
      "gun¨\n",
      "2007two\n",
      "particulare¨\n",
      "timesin\n",
      "doesn¡¦t\n",
      "directionand\n",
      "film\n",
      "\n",
      "¨invit\n",
      "filmmuch\n",
      "pointfirst\n",
      "betterthan\n",
      "outlet…but\n",
      "simplebut\n",
      "songmak\n",
      "mother\n",
      "oneswith\n",
      "biographiesi\n",
      "onecharact\n",
      "looksand\n",
      "‘\n",
      "n\n",
      "spawn\n",
      "sh¤tdefinit\n",
      "s\n",
      "muchchandu\n",
      "£1\n",
      "propsdodgi\n",
      "ø\n",
      "randomy\n",
      "andunlik\n",
      "q\n",
      "travestyat\n",
      "14\n",
      "¨call\n",
      "head\n",
      "richard\n",
      "babi\n",
      "stanislavski\n",
      "c\n",
      "point­\n",
      "₤250000\n",
      "a\n",
      "whofunni\n",
      "the\n",
      "£1000000\n",
      "musicth\n",
      "down´\n",
      "selfishalway\n",
      "li´\n",
      "4°c\n",
      "charactersth\n",
      "£6\n",
      "bc2006¨\n",
      "confusinghalperin\n",
      "escapeuntil\n",
      "lookalikethat\n",
      "cinemaa\n",
      "standardsmoder\n",
      "anthologya\n",
      "worldand\n",
      "imageryand\n",
      "sensit\n",
      "isabelwho\n",
      "brothera\n",
      "arguabl\n",
      "cameronin\n",
      "costumesso\n",
      "pictureh\n",
      "£199\n",
      "society®\n",
      "´83\n",
      "hasn´t\n",
      "sheetswhat\n",
      "ashamedar\n",
      "homosexualityand\n",
      "£300000\n",
      "pointlat\n",
      "cuttingal\n",
      "2\n",
      "mighti\n",
      "lincoln¨1930\n",
      "albertsomeon\n",
      "£18000000\n",
      "iek\n",
      "toa\n",
      "isn´t\n",
      "father¡¦\n",
      "wayand\n",
      "peacefulend\n",
      "christ´\n",
      "conserv\n",
      "won´t\n",
      "it¡¨\n",
      "elephantsfar\n",
      "oppressionrepres\n",
      "x\n",
      "that´\n",
      "vance¨1940th\n"
     ]
    }
   ],
   "source": [
    "teste = list()\n",
    "for word in vocab:\n",
    "    if word not in countvec.vocabulary_:\n",
    "        print(word)\n",
    "        teste.append(word)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
